---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

#### Article ID: UYYED
#### Pilot: Gustav Nilsonne
#### Co-pilot: Tom Hardwicke
#### Start date: 03/29/2017
#### End date:

-------

#### Methods summary: 
Infants were shown images of everyday objects with mono- or trisyllabic names. The infants' eye movements were recorded. There was a training phase and a test phase. In the training phase, images were first presented in the center of the screen and the name of the object was played as sound. Next, the objects were presented on one side. Objects with monosyllabic names were always presented on one side and objects with longer names on the other. In the test phase, infants were presented images of new objects with names they could comprehend but not produce. The object appeared first in the center, with no auditory stimulus, and then on one side, depending on the word length. The question was whether the infants would look to the side the image appeared on in anticipation.

------

#### Target outcomes: 
For this article we will focus on the findings reported for Experiment 1 in section 2.2. Specifically, we will attempt to reproduce all descriptive and inferential analyses reported in the text below and associated tables/figures:

> During the learning phase, infants showed at least one left or right look in 81% of the trials. Infants’ mean scores were not significantly above chance for either initial accuracy (mean = −0.02, SE = 0.03, t(30) < 1) or overall accuracy (mean = −0.002, SE = 0.03, t(30) < 1, ns). During the test phase, they showed at least one left or right look in 83% of the trials. The mean latency of the first look was 625 ms (SD: 141 ms), and its mean duration 819 ms (SD: 238 ms). Infants’ mean accuracy scores were significantly above chance, considering both initial accuracy (mean = 0.12, SE = 0.04, t(30) = 3.25, p = 0.0014; see Fig. 2, left panel) and overall accuracy (mean = 0.06, SE = 0.03, t(30) = 1.69, p = 0.050).

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(weights) # for weighted t-tests
sem <- function(x) {sd(x, na.rm=TRUE) / sqrt(length(x))} # custom function to calculate standard error of the mean
```

## Step 2: Load data

NB - we found errors and missing data in the data file shared with the original manuscript and contacted one of the original authors for assistance. They agreed that there were a number of problems with the original data file and sent a second data file intended to replace it. It is not clear why the errors arose in the original data file. Our reproducibility check will use the replacement data file.

```{r}
d <- read_excel("data/tri_final.xlsx", skip = 1)
```

## Step 3: Tidy data

```{r}
d <- d %>%
  mutate(scorefirstlook = as.numeric(scorefirstlook)) %>% # Change from character to numeric
  filter(pronounced == "no") # Exclude trials with pronounced words, see footnote on page 54
```

## Step 4: Run analysis

### Pre-processing

> During the learning phase, infants showed at least one left or right look in 81% of the trials

```{r}
a <- d %>%
  filter(phase =='train') %>%
  mutate(attempted = factor(attempted)) %>%
  select(attempted)

att_train <- length(a$attempted[a$attempted == 'yes'])/length(a$attempted)
compareValues(reportedValue = 81, obtainedValue = att_train*100)
```

> During the test phase, they showed at least one left or right look in 83% of the trials. 

```{r}
a <- d %>%
  filter(phase =='test') %>%
  mutate(attempted = factor(attempted)) %>%
  select(attempted)

att_test <- length(a$attempted[a$attempted == 'yes'])/length(a$attempted)
compareValues(reportedValue = 83, obtainedValue = att_test*100)
```

> Trials that contained neither a left nor a right look were excluded from the analyses

```{r}
d <- d %>% filter(attempted == 'yes')
```

For each participant, scores were calculated and scaled, and weights were determined for each participant depending on the number of trials with a recorded response (section 2.2, page 55):

> Two individual accuracy measures were computed based on infants’ left and right looks (hence excluding the other looks): initial accuracy, defined as proportion of first fixations to correct side, and overall accuracy, defined as proportion looking time to correct side. Both scores were scaled such that they ran from -1, corresponding to 100% incorrect anticipations, to +1, corresponding to 100% correct anticipations, with 0 representing performance at chance level. To test whether infants’ mean accuracy scores were significantly above chance, we ran onesided t-tests. Given that the number of analyzable test trials varied widely across infants (mean = 14.9; range = 4–27), both because the number of trials was variable and because infants did not necessarily show at least one left or right look in all trials, we used a weighted version. In this version, the contribution of individual infants’ accuracy scores to the t-statistic is proportional to their number of anticipated trials.

Let's check that the number of analyzable test trials matches:

```{r}
trials <- d %>% group_by(subj) %>%
  filter(phase == 'test') %>%
  summarise(n = n()) %>%
  summarise(mean(n), min(n), max(n))

kable(trials)
```

Very close, only the mean is very slightly off.

> The mean latency of the first look was 625 ms (SD: 141 ms), 

```{r}
firstLookLatency <- d %>% filter(phase == 'test') %>% summarise(M = mean(RT), SD = sd(RT))
kable(firstLookLatency)
```

Looks like the values do not match, let's compare them explictly:

```{r}
compareValues(reportedValue = 625, obtainedValue = firstLookLatency$M)
compareValues(reportedValue = 141, obtainedValue = firstLookLatency$SD)
```

> and its mean duration 819 ms (SD: 238 ms). 

Need to seek clarfification on how this was calculated.


Let's calculate the two accuracy measures:

```{r}
accTraining <- d %>% 
  filter(phase == "train") %>% # training phase
  mutate(totalTime = LTcongruent + LTincongruent) %>%
  group_by(subj) %>%
  summarise(initialAcc = sum(scorefirstlook, na.rm = T)/n(), 
            overallAcc = sum(LTcongruent)/sum(totalTime),
            n_obs = n()) %>%
  mutate(initialAccScaled = initialAcc * 2 - 1,
         overallAccScaled = overallAcc * 2 - 1)

accTest <- d %>% 
  filter(phase == "test") %>% # test phase
  mutate(totalTime = LTcongruent + LTincongruent) %>%
  group_by(subj) %>%
  summarise(initialAcc = sum(scorefirstlook, na.rm = T)/n(),
            overallAcc = sum(LTcongruent)/sum(totalTime),
            n_obs = n()) %>%
  mutate(initialAccScaled = initialAcc * 2 - 1,
         overallAccScaled = overallAcc * 2 - 1)

# summary stats
trainingStats <- accTraining %>% summarise(initial_M = mean(initialAccScaled), initial_SE = sem(initialAccScaled),
                                 overall_M = mean(overallAccScaled), overall_SE = sem(overallAccScaled))

testStats <- accTest %>% summarise(initial_M = mean(initialAccScaled), initial_SE = sem(initialAccScaled),
                                 overall_M = mean(overallAccScaled), overall_SE = sem(overallAccScaled))
```

For the training phase:

> Infants’ mean scores were not significantly above chance for either initial accuracy (mean = −0.02, SE = 0.03, t(30) < 1) 

```{r}
## for initial accuracy
compareValues(reportedValue = -0.02, obtainedValue = trainingStats$initial_M)
compareValues(reportedValue = 0.03, obtainedValue = trainingStats$initial_SE)
wtd.t.test(accTraining$initialAccScaled, weight = accTraining$n_obs, alternative = "greater")
# eyeballing indicates that the df and t value are accurately reported.
```

> or overall accuracy (mean = −0.002, SE = 0.03, t(30) < 1, ns). 

```{r}
## for overall accuracy
compareValues(reportedValue = -0.002, obtainedValue = trainingStats$overall_M)
compareValues(reportedValue = 0.03, obtainedValue = trainingStats$overall_SE)
wtd.t.test(accTraining$overallAccScaled, weight = accTraining$n_obs, alternative = "greater")
# eyeballing indicates that the df and t value are accurately reported.
```

For the test phase: 

> Infants’ mean accuracy scores were significantly above chance, considering both initial accuracy (mean = 0.12, SE = 0.04, t(30) = 3.25, p = 0.0014; see Fig. 2, left panel)...

```{r}
## for initial accuracy
compareValues(reportedValue = 0.12, obtainedValue = testStats$initial_M)
compareValues(reportedValue = 0.04, obtainedValue = testStats$initial_SE)
t.out <- wtd.t.test(accTest$initialAccScaled, weight = accTest$n_obs, alternative = "greater")
t.out
```

There seem to be some discrepancies with the t-test outcome, let's check them explicitly:

```{r}
# compare t-value
compareValues(reportedValue = 3.25, obtainedValue = t.out$coefficients[['t.value']])
# compare p-value
compareValues(reportedValue = 0.0014, obtainedValue = t.out$coefficients[['p.value']], isP = T)
```

> ...and overall accuracy (mean = 0.06, SE = 0.03, t(30) = 1.69, p = 0.050).

```{r}
## for overall accuracy
compareValues(reportedValue = 0.06, obtainedValue = testStats$overall_M)
compareValues(reportedValue = 0.03, obtainedValue = testStats$overall_SE)
t.out <- wtd.t.test(accTest$overallAccScaled, weight = accTest$n_obs, alternative = "greater")
t.out
```

There seem to be some discrepancies with the t-test outcome, let's check them explicitly:

```{r}
# compare t-value
compareValues(reportedValue = 1.69, obtainedValue = t.out$coefficients[['t.value']])
# compare p-value
compareValues(reportedValue = .05, obtainedValue = t.out$coefficients[['p.value']], isP = T)
```

## Step 5: Conclusion

```{r}
codReport(Report_Type = 'joint',
          Article_ID = 'UYYED', 
          Insufficient_Information_Errors = 0,
          Decision_Errors = 0, 
          Major_Numerical_Errors = 11, 
          Minor_Numerical_Errors = 3)
```



```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
