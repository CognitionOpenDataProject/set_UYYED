---
title: "COD Reproducibility Report"
output:
  html_document:
    toc: true
    toc_float: true
---

#### Article ID: UYYED
#### Pilot: Gustav Nilsonne
#### Co-pilot: Tom Hardwicke
#### Start date: Mar 29 2017
#### End date: Mar 24 2017
#### Final verification: Tom Hardwicke
#### Date: Nov 13 2017

-------

#### Methods summary: 
Infants were shown images of everyday objects with mono- or trisyllabic names. The infants' eye movements were recorded. There was a training phase and a test phase. In the training phase, images were first presented in the center of the screen and the name of the object was played as sound. Next, the objects were presented on one side. Objects with monosyllabic names were always presented on one side and objects with longer names on the other. In the test phase, infants were presented images of new objects with names they could comprehend but not produce. The object appeared first in the center, with no auditory stimulus, and then on one side, depending on the word length. The question was whether the infants would look to the side the image appeared on in anticipation.

------

#### Target outcomes: 
For this article we will focus on the findings reported for Experiment 1 in section 2.2. Specifically, we will attempt to reproduce all descriptive and inferential analyses reported in the text below and associated tables/figures:

> During the learning phase, infants showed at least one left or right look in 81% of the trials. Infants’ mean scores were not significantly above chance for either initial accuracy (mean = −0.02, SE = 0.03, t(30) < 1) or overall accuracy (mean = −0.002, SE = 0.03, t(30) < 1, ns). During the test phase, they showed at least one left or right look in 83% of the trials. The mean latency of the first look was 625 ms (SD: 141 ms), and its mean duration 819 ms (SD: 238 ms). Infants’ mean accuracy scores were significantly above chance, considering both initial accuracy (mean = 0.12, SE = 0.04, t(30) = 3.25, p = 0.0014; see Fig. 2, left panel) and overall accuracy (mean = 0.06, SE = 0.03, t(30) = 1.69, p = 0.050).

------

```{r global_options, include=FALSE}
knitr::opts_chunk$set(echo=TRUE, warning=FALSE, message=FALSE)

# prepare an empty report object, we will update this each time we run compareValues2()
reportObject <- data.frame("Article_ID" = NA, "valuesChecked" = 0, "eyeballs" = 0, "Total_df" = 0, "Total_p" = 0, "Total_mean" = 0, "Total_sd" = 0, "Total_se" = 0, "Total_ci" = 0, "Total_bf" = 0, "Total_t" = 0, "Total_F" = 0, "Total_es" = 0, "Total_median" = 0, "Total_irr" = 0, "Total_r" = 0, "Total_z" = 0, "Total_coeff" = 0, "Total_n" = 0, "Total_x2" = 0, "Total_other" = 0, "Insufficient_Information_Errors" = 0, "Decision_Errors" = 0, "Major_Numerical_Errors" = 0, "Minor_Numerical_Errors" = 0, "Major_df" = 0, "Major_p" = 0, "Major_mean" = 0, "Major_sd" = 0, "Major_se" = 0, "Major_ci" = 0, "Major_bf" = 0, "Major_t" = 0, "Major_F" = 0, "Major_es" = 0, "Major_median" = 0, "Major_irr" = 0, "Major_r" = 0, "Major_z" = 0, "Major_coeff" = 0, "Major_n" = 0, "Major_x2" = 0, "Major_other" = 0, "affectsConclusion" = NA, "error_typo" = 0, "error_specification" = 0, "error_analysis" = 0, "error_data" = 0, "error_unidentified" = 0, "Author_Assistance" = NA, "resolved_typo" = 0, "resolved_specification" = 0, "resolved_analysis" = 0, "resolved_data" = 0, "correctionSuggested" = NA, "correctionPublished" = NA)
```

## Step 1: Load packages

```{r}
library(tidyverse) # for data munging
library(knitr) # for kable table formating
library(haven) # import and export 'SPSS', 'Stata' and 'SAS' Files
library(readxl) # import excel files
library(CODreports) # custom report functions
library(weights) # for weighted t-tests
sem <- function(x) {sd(x, na.rm=TRUE) / sqrt(length(x))} # custom function to calculate standard error of the mean
```

## Step 2: Load data

NB - we found errors and missing data in the data file shared with the original manuscript and contacted one of the original authors for assistance (S.P.). They agreed that there were a number of problems with the original data file and sent a second data file intended to replace it. It is not clear why the errors arose in the original data file. Our reproducibility check will use the replacement data file.

```{r}
d <- read_excel("data/tri_final.xlsx", skip = 1)
```

## Step 3: Tidy data

```{r}
d <- d %>%
  mutate(scorefirstlook = as.numeric(scorefirstlook)) %>% # Change from character to numeric
  filter(pronounced == "no") # Exclude trials with pronounced words, see footnote on page 54
```

## Step 4: Run analysis

### Pre-processing

> During the learning phase, infants showed at least one left or right look in 81% of the trials

```{r}
a <- d %>%
  filter(phase =='train') %>%
  mutate(attempted = factor(attempted)) %>%
  select(attempted)

att_train <- length(a$attempted[a$attempted == 'yes'])/length(a$attempted)
reportObject <- compareValues2(reportedValue = "81", obtainedValue = att_train*100, valueType = 'n')
```

> During the test phase, they showed at least one left or right look in 83% of the trials. 

```{r}
a <- d %>%
  filter(phase =='test') %>%
  mutate(attempted = factor(attempted)) %>%
  select(attempted)

att_test <- length(a$attempted[a$attempted == 'yes'])/length(a$attempted)
reportObject <- compareValues2(reportedValue = "83", obtainedValue = att_test*100, valueType = 'n')
```

> Trials that contained neither a left nor a right look were excluded from the analyses

```{r}
d <- d %>% filter(attempted == 'yes')
```

For each participant, scores were calculated and scaled, and weights were determined for each participant depending on the number of trials with a recorded response (section 2.2, page 55):

> Two individual accuracy measures were computed based on infants’ left and right looks (hence excluding the other looks): initial accuracy, defined as proportion of first fixations to correct side, and overall accuracy, defined as proportion looking time to correct side. Both scores were scaled such that they ran from -1, corresponding to 100% incorrect anticipations, to +1, corresponding to 100% correct anticipations, with 0 representing performance at chance level. To test whether infants’ mean accuracy scores were significantly above chance, we ran onesided t-tests. Given that the number of analyzable test trials varied widely across infants (mean = 14.9; range = 4–27), both because the number of trials was variable and because infants did not necessarily show at least one left or right look in all trials, we used a weighted version. In this version, the contribution of individual infants’ accuracy scores to the t-statistic is proportional to their number of anticipated trials.

Let's check that the number of analyzable test trials matches:

```{r}
trials <- d %>% group_by(subj) %>%
  filter(phase == 'test') %>%
  summarise(n = n()) %>%
  summarise(mean(n), min(n), max(n))

reportObject <- compareValues2(reportedValue = "14.9", obtainedValue = trials$`mean(n)`, valueType = 'mean')
reportObject <- compareValues2(reportedValue = "4", obtainedValue = trials$`min(n)`, valueType = 'n')
reportObject <- compareValues2(reportedValue = "27", obtainedValue = trials$`max(n)`, valueType = 'n')
```

Very close, only the mean is very slightly off.

NB. The original author has noted that the value reported in the article is wrong (it was retained from a previous version and not updated) and the value we obtained here is the correct value.

> The mean latency of the first look was 625 ms (SD: 141 ms), 

```{r}
firstLookLatency <- d %>% 
  filter(phase == 'test') %>% 
  group_by(subj) %>%
  summarise(subMean = mean(RT)) %>%
  summarise(M = mean(subMean), SD = sd(subMean))
kable(firstLookLatency)
```

Let's compare the values explictly:

```{r}
reportObject <- compareValues2(reportedValue = "625", obtainedValue = firstLookLatency$M, valueType = 'mean')
reportObject <- compareValues2(reportedValue = "141", obtainedValue = firstLookLatency$SD, valueType = 'sd')
```

Let's calculate the two accuracy measures:

```{r}
accTraining <- d %>% 
  filter(phase == "train") %>% # training phase
  mutate(totalTime = LTcongruent + LTincongruent) %>%
  group_by(subj) %>%
  summarise(initialAcc = sum(scorefirstlook, na.rm = T)/n(), 
            overallAcc = sum(LTcongruent)/sum(totalTime),
            n_obs = n()) %>%
  mutate(initialAccScaled = initialAcc * 2 - 1,
         overallAccScaled = overallAcc * 2 - 1)

accTest <- d %>% 
  filter(phase == "test") %>% # test phase
  mutate(totalTime = LTcongruent + LTincongruent) %>%
  group_by(subj) %>%
  summarise(initialAcc = sum(scorefirstlook, na.rm = T)/n(),
            overallAcc = sum(LTcongruent)/sum(totalTime),
            n_obs = n()) %>%
  mutate(initialAccScaled = initialAcc * 2 - 1,
         overallAccScaled = overallAcc * 2 - 1)

# summary stats
trainingStats <- accTraining %>% summarise(initial_M = mean(initialAccScaled), initial_SE = sem(initialAccScaled),
                                 overall_M = mean(overallAccScaled), overall_SE = sem(overallAccScaled))

testStats <- accTest %>% summarise(initial_M = mean(initialAccScaled), initial_SE = sem(initialAccScaled),
                                 overall_M = mean(overallAccScaled), overall_SE = sem(overallAccScaled))
```

For the training phase:

> Infants’ mean scores were not significantly above chance for either initial accuracy (mean = −0.02, SE = 0.03, t(30) < 1) 

```{r}
## for initial accuracy
reportObject <- compareValues2(reportedValue = "-0.02", obtainedValue = trainingStats$initial_M, valueType = 'mean')
reportObject <- compareValues2(reportedValue = "0.03", obtainedValue = trainingStats$initial_SE, valueType = 'se')

t.out <- wtd.t.test(accTraining$initialAccScaled, weight = accTraining$n_obs, alternative = "greater")
reportObject <- compareValues2(reportedValue = "30", obtainedValue = t.out$coefficients[2], valueType = 'df')
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = t.out$coefficients[1], valueType = 't')
```

> or overall accuracy (mean = −0.002, SE = 0.03, t(30) < 1, ns). 

```{r}
## for overall accuracy
reportObject <- compareValues2(reportedValue = "-0.002", obtainedValue = trainingStats$overall_M, valueType = 'mean')
reportObject <- compareValues2(reportedValue = "0.03", obtainedValue = trainingStats$overall_SE, valueType = 'se')

t.out <- wtd.t.test(accTraining$overallAccScaled, weight = accTraining$n_obs, alternative = "greater")
reportObject <- compareValues2(reportedValue = "30", obtainedValue = t.out$coefficients[2], valueType = 'df')
reportObject <- compareValues2(reportedValue = "eyeballMATCH", obtainedValue = t.out$coefficients[1], valueType = 't')
```

For the test phase: 

> Infants’ mean accuracy scores were significantly above chance, considering both initial accuracy (mean = 0.12, SE = 0.04, t(30) = 3.25, p = 0.0014; see Fig. 2, left panel)...

```{r}
## for initial accuracy
reportObject <- compareValues2(reportedValue = "0.12", obtainedValue = testStats$initial_M, valueType = 'mean')
reportObject <- compareValues2(reportedValue = "0.04", obtainedValue = testStats$initial_SE, valueType = 'se')

t.out <- wtd.t.test(accTest$initialAccScaled, weight = accTest$n_obs, alternative = "greater")
reportObject <- compareValues2(reportedValue = "30", obtainedValue = t.out$coefficients[2], valueType = 'df')
reportObject <- compareValues2(reportedValue = "3.25", obtainedValue = t.out$coefficients[1], valueType = 't')
reportObject <- compareValues2(reportedValue = "0.0014", obtainedValue = t.out$coefficients[3], valueType = 'p')
```

There seem to be some discrepancies here.

NB one of the the original authors has confirmed there is an issue here. Apparently the reported values were based on an old dataset. The values we obtained here are the correct values.

> ...and overall accuracy (mean = 0.06, SE = 0.03, t(30) = 1.69, p = 0.050).

```{r}
## for overall accuracy
reportObject <- compareValues2(reportedValue = "0.06", obtainedValue = testStats$overall_M, valueType = 'mean')
reportObject <- compareValues2(reportedValue = "0.03", obtainedValue = testStats$overall_SE, valueType = 'se')

t.out <- wtd.t.test(accTest$overallAccScaled, weight = accTest$n_obs, alternative = "greater")
reportObject <- compareValues2(reportedValue = "30", obtainedValue = t.out$coefficients[2], valueType = 'df')
reportObject <- compareValues2(reportedValue = "1.69", obtainedValue = t.out$coefficients[1], valueType = 't')

# NB - the authors actually interpret p = .05 as statistically significant. As the correct value is .04, this description is accurate, so we will not consider this a decision error, only a major numerical error. We need to 'trick' the function a little here - we cannot enter 0.05 as it will record a decision error. We enter 0.049 instead - which still correctly records that there is a major numerical error.
reportObject <- compareValues2(reportedValue = "0.049", obtainedValue = t.out$coefficients[3], valueType = 'p')
```

There seem to be some discrepancies here too.

NB one of the the original authors has confirmed there is an issue here. Apparently the reported values were based on an old dataset. The values we obtain here are the correct values.

## Step 5: Conclusion

We encountered many reproducibility issues here. We contacted the original authors for assistance and were told that there were a large number of errors in the data file itself. These appeared to have been introduced during manual editing of the data file in order to make it more reader friendly. Additionally, when they checked their own analyses, the authors found that some of the values reported in the paper were based on a previous version of the data set and had not been updated. The author also provided some additional information about data aggregation which helped to resolve some errors.

```{r}
reportObject$Article_ID <- "UYYED"
reportObject$affectsConclusion <- "no"
reportObject$error_typo <- 0
reportObject$error_specification <- 0
reportObject$error_analysis <- 1
reportObject$error_data <- 0
reportObject$error_unidentified <- 0
reportObject$Author_Assistance <- T
reportObject$resolved_typo <- 0
reportObject$resolved_specification <- 1
reportObject$resolved_analysis <- 0
reportObject$resolved_data <- 1
reportObject$correctionSuggested <- "yes"
reportObject$correctionPublished <- T

# decide on final outcome
if(reportObject$Decision_Errors > 0 | reportObject$Major_Numerical_Errors > 0 | reportObject$Insufficient_Information_Errors > 0){
  reportObject$finalOutcome <- "Failure"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Failure despite author assistance"
  }
}else{
  reportObject$finalOutcome <- "Success"
  if(reportObject$Author_Assistance == T){
    reportObject$finalOutcome <- "Success with author assistance"
  }
}

# save the report object
filename <- paste0("reportObject_", reportObject$Article_ID,".csv")
write_csv(reportObject, filename)
```

## Report Object

```{r, echo = FALSE}
# display report object in chunks
kable(reportObject[2:10], align = 'l')
kable(reportObject[11:20], align = 'l')
kable(reportObject[21:25], align = 'l')
kable(reportObject[26:30], align = 'l')
kable(reportObject[31:35], align = 'l')
kable(reportObject[36:40], align = 'l')
kable(reportObject[41:45], align = 'l')
kable(reportObject[46:51], align = 'l')
kable(reportObject[52:57], align = 'l')
```

## Session information

```{r session_info, include=TRUE, echo=TRUE, results='markup'}
devtools::session_info()
```
